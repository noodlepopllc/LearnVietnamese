{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noodlepopllc/LearnVietnamese/blob/main/Colab/Translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Imports**\n",
        "\n",
        "First import models needed\n",
        " - torch https://pytorch.org/\n",
        " - transformers https://github.com/huggingface/transformers\n",
        "\n",
        " The model we are going to be using is based on T5 https://huggingface.co/docs/transformers/model_doc/t5"
      ],
      "metadata": {
        "id": "UYQa6KJTn-eH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JDhlHQGI2DHa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Get Device**\n",
        "\n",
        "Check to see if cuda is available for hardware acceleration otherwise use cpu for inferencing"
      ],
      "metadata": {
        "id": "ULooGXJeozf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqvOqWW87jto",
        "outputId": "6d7ae5dd-d590-4453-dc73-bb1b7d5c3d9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load the Model**\n",
        "\n",
        "The model we are using comes from VietAI this is an open source english to vietnamese t5 translator, it is quite fast and runs well even on cpu\n",
        "\n",
        "https://huggingface.co/VietAI/envit5-translation\n",
        "\n"
      ],
      "metadata": {
        "id": "UvIk_Ia_pTRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"VietAI/envit5-translation\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "kIfN02fL747N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Translation**\n",
        "\n",
        "First need a list of inputs, this model can handle both English to Vietnamese and Vietnamese to English. Each item in the list must start with either vi: or en: to notify the model of the input language.\n",
        "\n",
        "Next the tokenizer will take the inputs and convert them into pytorch format and copy them to the device. The tokens must be in the same memory as the model. Finally the oposite has to take place must take the output tokens and covert them back into words. The output generated is same as the input, prepended with vi: or en: depending on language.\n"
      ],
      "metadata": {
        "id": "IfRMo1CSqswr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Example of Vietnamese to English**"
      ],
      "metadata": {
        "id": "_6ljZxUPsPx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = ['vi: Xin chào thế giới']\n",
        "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to(device), max_length=512)\n",
        "print([output for output in tokenizer.batch_decode(outputs, skip_special_tokens=True)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLOXUD11_mH3",
        "outputId": "25c3d270-30cb-48a2-c1e1-857e4f17b146"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['en: Hello world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Example of English to Vietnamese**"
      ],
      "metadata": {
        "id": "zE3-0bcOsH7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\"en: Hello world\"]\n",
        "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to(device), max_length=512)\n",
        "print([output for output in tokenizer.batch_decode(outputs, skip_special_tokens=True)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzohd8T1_W7A",
        "outputId": "3bd721dc-6126-471b-db74-139ab4a71fab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vi: Xin chào thế giới']\n"
          ]
        }
      ]
    }
  ]
}